```
python3 \
 -m llama_cpp.server \
 --model $MODEL \
 --n_gpu_layers 1 \
 --port 19000
```
